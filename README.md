# Week-7-Lecture
Week-7-Lecture

### AlexNet은 어떻게 딥러닝의 시대를 열었을까?
* 논문명: ImageNet Classification with Deep Convolutional Neural Networks(깊은 컨볼루션 신경망을 사용한 ImageNet 분류)
* AlexNet은 이미지 인식의 판도를 어떻게 뒤집었는가?
### 논문배경
* 객체 인식의 난이도 증가: 현실 세계의 객체들은 변동성이 커서 이를 인식하기 위해서는 훨씬 더 큰 훈련 데이터셋이 필요했음
* 새로운 대규모 데이터셋 등장: ImageNet 과 같은 수백만 개의 레이블링된 고해상도 이미지를 포함하는 대규모 데이터셋이 등장함.
* 대용량 모델의 필요성: 수백만 장의 이미지에서 수천 개의 객체를 학습하기 위해서는 큰 학습 용량(large learning capacity)을 가진 모델이 필요했음.
* GPU 발전: 최신 GPU와 고도로 최적화된 2D 컨볼루션 구현 덕분에, 오버피팅 없이 대규모 컨볼루션 신경망(CNN)을 훈련시키는 것이 가능해졌음.

### 기존 연구의 한계점(CNN 사용의 어려움)
* 계산 비용: CNN은 매력적인 특성에도 불구하고, 고해상도 이미지에 대규모로 적용하기에는 여전히 계산 비용이 지나치게 높았음 (prohibitively expensive).
* 작은 데이터셋: 이전에는 레이블링된 이미지 데이터셋이 수만 개 정도로 상대적으로 작았기 때문에 , 현실적인 객체 인식에 필요한 대규모 모델을 훈련하기 어려웠음.

### 논문 목표
* ImageNet 대규모 이미지 분류 문제 해결
     * ImageNet LSVRC-2010 대회에 출품된 1.2백만 개의 고해상도 이미지를 1,000개의 다른 클래스로 분류하기 위해 크고 깊은 컨볼루션 신경망(CNN)을 훈련하는 것
* 당시 최고의 성능 달성
     * 기존의 최고 수준(state-of-the-art)보다 훨씬 더 나은top-1 및 top-5 오류율을 달성하고 기록을 깨는 것 
* 대규모 딥러닝 모델의 실현 가능성과 효과 입증
     * ReLU 비선형성, GPU 병렬 처리, Dropout 등 여러 혁신적인 기술을 적용하여, 대규모의 깊은 CNN이 복잡한 이미지 분류 작업에서 순수 지도 학습만으로도 탁월한 성능을 낼 수 있음을 입증하는 것

### 논문내용(주요 혁신 및 방법론)
* 모델 구조 (Architecture)
   * 깊은 구조: 5개의 컨볼루션 계층과 3개의 완전 연결 계층, 총 8개의 학습 가능한 계층을 가진 대규모 CNN을 사용했습니다.
   * GPU 병렬 처리: 하나의 GPU 메모리로는 네트워크를 훈련시키기에 너무 커서, 두 개의 GPU에 네트워크를 분산하여 훈련했습니다.
* 훈련 속도 개선
   * ReLU 비선형성 + 효율적인 GPU
   * 오퍼피팅(Overfitting) 방지: Dropout, 데이터증강(Data Augmentation)
* 일반화 성능 향상
   * 지역응답 정규화(LRN)
   * 중첩 풀링(Overlapping Pooling)

### 기여도 및 결과
* 경쟁 우위 확보: ILSVRC-2010 대회에서 기존 최고 기록(Top-5 오류율 28.2%)을 훨씬 능가하는 Top-5 오류율 **17.0%**를 달성.
* 대회 우승: ILSVRC-2012 대회에서도 압도적인 차이(2등 26.2% 대비)로 Top-5 테스트 오류율 15.3%를 기록하며 우승.
* 깊이의 중요성 입증: 단 하나의 컨볼루션 계층만 제거해도 성능이 저하된다는 것을 보여주어, 깊은 네트워크 구조의 중요성 입증.
* 딥러닝의 부활: 대규모 CNN이 순수 지도 학습만으로도 매우 어려운 데이터셋에서 기록적인 결과를 달성할 수 있음을 보여주며, 이후 딥러닝 연구의 폭발적인 성장을 이끎

### 참고 파일

* 논문1. ImageNet Classification with Deep Convolutional Neural Networks
* 코드1. CiFAR-10 분류를 위한 CNN 모델
* 코드해설 1. CiFAR-10 분류를 위한 CNN 모델 해설
* CNN+합성곱 예시. 
